import{_ as p}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c,d as t,b as o,a as r,f as n,h as a,r as m,o as g,e as u}from"./app-CkR019O1.js";const f="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_acf.png",y="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_pacf.png",b="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.ARMA_Params.png",A="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_zt_acf.png",v="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_zt_pacf.png",w="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.arch_effect_acf.png",_="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.arch_effect_pacf.png",x="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_zt_squared_acf.png",q="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_zt_squared_pacf.png",k="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.ARCH_Params.png",F="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.ARCH_CoverageRate.png",R="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.GARCH_Params.png",z="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.GARCH_CoverageRate.png",d="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.Model2011To2012.png",C="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.Model2011To2012.acc90.png",j="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.statisticalfor2012Zt.png",M="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.quantile.png",T="/data/unisa/AdvancedAnalytic1/assignment2/img/q1.quantile_ret.png",D="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.histogram.png",J="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.pp-plot.png",H="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.ParametersForGamma.png",G="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.gammadistribution.png",P="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.gammatest.png",E="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.gamma-Jan.png",S="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.gamma-Feb.png",Q="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.gamma-Dec.png",Z="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.correlations.png",L="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.PearsonCorrelationsMatrix.png",B="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.generate_data_JanFebDec.png",I="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.maxmin_JanFebDec.png",O="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.freq_JanFebDec.png",V="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.accfreq_Jan.png",N="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.accfreq_Feb.png",U="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.accfreq_Dec.png",W="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.pp-plot_JanFebDec.png",$="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_Jan.png",K="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_Feb.png",X="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_Dec.png",Y="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_JanFebDec.png",tt="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.generate_data_JulAug.png",at="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.maxmin_JulAug.png",et="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.freq_JulAug.png",nt="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.accfreq_Jul.png",st="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.accfreq_Aug.png",it="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ppplot_JulyAug.png",ot="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_JulyAug.png",lt="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_Jul.png",rt="/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_Aug.png",mt="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.DFT_power.png",dt="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.seasonality.png",ht="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.seasonality_visualization.png",pt="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_data.png",ct="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_alpha_0.02.png",gt="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_alpha_0.05.png",ut="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_alpha_0.1.png",ft="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_alpha_0.15.png",yt="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_alpha_0.2.png",bt="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.total_trends.png",At="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.minitab.smoothing_alpha_0.02.png",vt="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.minitab.smoothing_alpha_0.05.png",wt="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.minitab.smoothing_alpha_0.1.png",_t="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.minitab.smoothing_alpha_0.15.png",xt="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.minitab.smoothing_alpha_0.2.png",qt="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_total_trend.png",kt="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_multisection.png",Ft="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.VisualizationOfTrends.png",Rt="/data/unisa/AdvancedAnalytic1/assignment2/img/q3.temperature-trend.png",zt={},Ct={class:"hint-container details"};function jt(Mt,e){const h=m("PDF"),l=m("Tabs");return g(),c("div",null,[e[26]||(e[26]=t("h2",{id:"requirements",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#requirements"},[t("span",null,"Requirements")])],-1)),t("details",Ct,[e[0]||(e[0]=t("summary",null,"Instructions",-1)),o(h,{url:"/data/unisa/AdvancedAnalytic1/assignment2/Assignment2.pdf",ratio:"1"})]),e[27]||(e[27]=r('<h2 id="requirements-analysis" tabindex="-1"><a class="header-anchor" href="#requirements-analysis"><span>Requirements Analysis</span></a></h2><details class="hint-container details"><summary>Details</summary><p><a href="https://lo.unisa.edu.au/mod/folder/view.php?id=3409063" target="_blank" rel="noopener noreferrer">Access Resources</a><br><a href="/data/unisa/AdvancedAnalytic1/assignment2/assign2_report.docx">Access Resources-FR</a></p></details>',2)),o(l,{id:"15",data:[{id:'Quesion 1 <span style="color:red;font-weight:bold;">(15 marks)</span>'},{id:'Question 2 <span style="color:red;font-weight:bold;">(15 marks)</span>'},{id:'Question 3 <span style="color:red;font-weight:bold;">(10 marks)</span>'}]},{title0:n(({value:s,isActive:i})=>e[1]||(e[1]=[a("Quesion 1 "),t("span",{style:{color:"red","font-weight":"bold"}},"(15 marks)",-1)])),title1:n(({value:s,isActive:i})=>e[2]||(e[2]=[a("Question 2 "),t("span",{style:{color:"red","font-weight":"bold"}},"(15 marks)",-1)])),title2:n(({value:s,isActive:i})=>e[3]||(e[3]=[a("Question 3 "),t("span",{style:{color:"red","font-weight":"bold"}},"(10 marks)",-1)])),tab0:n(({value:s,isActive:i})=>e[4]||(e[4]=[t("p",null,[a("01."),t("a",{href:"/data/unisa/AdvancedAnalytic1/assignment2/ClementsGapWindFarmOutput.xlsx"},[a("Files Required: "),t("span",{style:{color:"orange","font-weight":"bold"}},"ClementsGapWindFarmOutput.xlsx")])],-1),t("p",null,[a("The "),t("em",null,[t("strong",null,"tasks")]),a(" for this question are listed below.")],-1),t("ol",null,[t("li",null,[a("Take the 2011 output (the training set) and find the best "),t("span",{style:{color:"orange","font-weight":"bold"}},[t("em",null,"ARMA(p,q)")]),a(" model for the data.")]),t("li",null,[a("Take the noise "),t("span",{style:{color:"orange","font-weight":"bold"}},[t("em",null,[t("span",{class:"katex"},[t("span",{class:"katex-mathml"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("semantics",null,[t("mrow",null,[t("msub",null,[t("mi",null,"Z"),t("mi",null,"t")])]),t("annotation",{encoding:"application/x-tex"},"Z_t")])])]),t("span",{class:"katex-html","aria-hidden":"true"},[t("span",{class:"base"},[t("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),t("span",{class:"mord"},[t("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"Z"),t("span",{class:"msupsub"},[t("span",{class:"vlist-t vlist-t2"},[t("span",{class:"vlist-r"},[t("span",{class:"vlist",style:{height:"0.2806em"}},[t("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[t("span",{class:"pstrut",style:{height:"2.7em"}}),t("span",{class:"sizing reset-size6 size3 mtight"},[t("span",{class:"mord mathnormal mtight"},"t")])])]),t("span",{class:"vlist-s"},"​")]),t("span",{class:"vlist-r"},[t("span",{class:"vlist",style:{height:"0.15em"}},[t("span")])])])])])])])])])]),a(" from that model and check its "),t("span",{style:{color:"orange","font-weight":"bold"}},[t("em",null,"SACF")]),a(".")]),t("li",null,[a("Calculate "),t("span",{style:{color:"orange","font-weight":"bold"}},[t("em",null,[t("span",{class:"katex"},[t("span",{class:"katex-mathml"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("semantics",null,[t("mrow",null,[t("msubsup",null,[t("mi",null,"Z"),t("mi",null,"t"),t("mn",null,"2")])]),t("annotation",{encoding:"application/x-tex"},"Z_t^2")])])]),t("span",{class:"katex-html","aria-hidden":"true"},[t("span",{class:"base"},[t("span",{class:"strut",style:{height:"1.0611em","vertical-align":"-0.247em"}}),t("span",{class:"mord"},[t("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"Z"),t("span",{class:"msupsub"},[t("span",{class:"vlist-t vlist-t2"},[t("span",{class:"vlist-r"},[t("span",{class:"vlist",style:{height:"0.8141em"}},[t("span",{style:{top:"-2.453em","margin-left":"-0.0715em","margin-right":"0.05em"}},[t("span",{class:"pstrut",style:{height:"2.7em"}}),t("span",{class:"sizing reset-size6 size3 mtight"},[t("span",{class:"mord mathnormal mtight"},"t")])]),t("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[t("span",{class:"pstrut",style:{height:"2.7em"}}),t("span",{class:"sizing reset-size6 size3 mtight"},[t("span",{class:"mord mtight"},"2")])])]),t("span",{class:"vlist-s"},"​")]),t("span",{class:"vlist-r"},[t("span",{class:"vlist",style:{height:"0.247em"}},[t("span")])])])])])])])])])]),a(" and show that it has the "),t("span",{style:{color:"orange","font-weight":"bold"}},[t("em",null,"ARCH")]),a(" effect.")]),t("li",null,[a("Find the best "),t("span",{style:{color:"orange","font-weight":"bold"}},[t("em",null,"ARCH")]),a(" or "),t("span",{style:{color:"orange","font-weight":"bold"}},[t("em",null,"GARCH")]),a(" model for it.")]),t("li",null,[a("Take the "),t("span",{style:{color:"orange","font-weight":"bold"}},"developed models"),a(" for "),t("span",{style:{color:"orange","font-weight":"bold"}},"the output"),a(" and also for "),t("span",{style:{color:"orange","font-weight":"bold"}},"the noise"),a(" and apply them to "),t("span",{style:{color:"orange","font-weight":"bold"}},"the 2012 output data"),a(".")]),t("li",null,[t("span",{style:{color:"orange","font-weight":"bold"}},"Evaluate the performance"),a(" of the models for "),t("span",{style:{color:"orange","font-weight":"bold"}},"one step ahead forecasting"),a(" with "),t("span",{style:{color:"orange","font-weight":"bold"}},"error bounds"),a(" by "),t("span",{style:{color:"orange","font-weight":"bold"}},"calculating the coverage and mean prediction interval width"),a(", for both 90% and 95% values.")]),t("li",null,[t("span",{style:{color:"orange","font-weight":"bold"}},"Compare the results"),a(" with "),t("span",{style:{color:"orange","font-weight":"bold"}},"constructing the prediction intervals by using the appropriate quantiles"),a(".")])],-1)])),tab1:n(({value:s,isActive:i})=>e[5]||(e[5]=[t("p",null,[a("01."),t("a",{href:"/data/unisa/AdvancedAnalytic1/assignment2/MelbourneAirportRain.xlsx"},[a("Files Required: "),t("span",{style:{color:"orange","font-weight":"bold"}},"MelbourneAirportRain.xlsx")])],-1),t("p",null,[a("The "),t("em",null,[t("strong",null,"tasks")]),a(" for this question are listed below.")],-1),t("ol",null,[t("li",null,[a("Test the months December, January, February, July, August for "),t("span",{style:{color:"orange","font-weight":"bold"}},"normality"),a(".")]),t("li",null,[a("For the months that "),t("span",{style:{color:"orange","font-weight":"bold"}},[a("do not follow a "),t("span",{style:{color:"red"}},"normal distribution"),a(", test for a "),t("span",{style:{color:"red"}},"Gamma fit")]),a(".")]),t("li",null,[a("Test December, January, February for "),t("span",{style:{color:"orange","font-weight":"bold"}},"correlation"),a(", and July, August separately.")]),t("li",null,[a("Generate 1000 years of "),t("span",{style:{color:"orange","font-weight":"bold"}},"synthetic"),a(" December, January, February, add the months to "),t("span",{style:{color:"orange","font-weight":"bold"}},"get seasonal totals"),a(", and "),t("span",{style:{color:"orange","font-weight":"bold"}},[a("generate empirical "),t("span",{style:{color:"red"}},"CDFs"),a(" for the totals versus the "),t("span",{style:{color:"red"}},"CDFs"),a(" for the real data")]),a(".")]),t("li",null,[t("span",{style:{color:"orange","font-weight":"bold"}},"Do the same"),a(" for July, August.")])],-1)])),tab2:n(({value:s,isActive:i})=>e[6]||(e[6]=[t("p",null,[a("01."),t("a",{href:"/data/unisa/AdvancedAnalytic1/assignment2/MelbourneAirportRain.xlsx"},[a("Files Required: "),t("span",{style:{color:"orange","font-weight":"bold"}},"MtGambierByMonthsTemperature.xlsx")]),t("br"),a(" 02."),t("a",{href:"/data/unisa/AdvancedAnalytic1/assignment2/MelbourneAirportRain.xlsx"},[a("Files Required: "),t("span",{style:{color:"orange","font-weight":"bold"}},"MtGambierRainfall.xlsx")])],-1),t("p",null,[a("The "),t("em",null,[t("strong",null,"tasks")]),a(" for this question are listed below.")],-1),t("ol",null,[t("li",null,[a("Take the monthly rainfall data from "),t("code",null,"MtGambierRainfall.xlsx"),a(" and "),t("span",{style:{color:"orange","font-weight":"bold"}},"model the seasonality"),a(". Then "),t("span",{style:{color:"orange","font-weight":"bold"}},"subtract"),a(" this from the data.")]),t("li",null,[a("Use "),t("span",{style:{color:"orange","font-weight":"bold"}},"exponential smoothing"),a(" to see "),t("span",{style:{color:"orange","font-weight":"bold"}},"the overall trend"),a(" in the series - "),t("span",{style:{color:"orange","font-weight":"bold"}},[a("try various values of "),t("code",null,"α"),a(" below 0.2")]),a(".")]),t("li",null,[t("span",{style:{color:"orange","font-weight":"bold"}},"Find the trend"),a(" for the whole series for "),t("span",{style:{color:"orange","font-weight":"bold"}},"the smoothed data"),a(", and then "),t("span",{style:{color:"orange","font-weight":"bold"}},"find the trends for any sections that you think display differing characteristics"),a(".")]),t("li",null,[a("Take the data for the "),t("span",{style:{color:"orange","font-weight":"bold"}},"month of December"),a(" and the "),t("span",{style:{color:"orange","font-weight":"bold"}},"Annual mean temperature"),a(" from "),t("code",null,"MtGambierByMonthsTemperature.xlsx"),a(" and "),t("span",{style:{color:"orange","font-weight":"bold"}},"find the trend over time"),a(".")]),t("li",null,[a("How much has "),t("span",{style:{color:"orange","font-weight":"bold"}},"the mean temperature changed"),a(" over time in each case?")])],-1)])),_:1}),e[28]||(e[28]=r('<hr><h2 id="all-works-below" tabindex="-1"><a class="header-anchor" href="#all-works-below"><span>All works below</span></a></h2><h3 id="question-1" tabindex="-1"><a class="header-anchor" href="#question-1"><span>Question 1</span></a></h3><h4 id="_1-take-the-2011-output-the-training-set-and-find-the-best-arma-p-q-model-for-the-data" tabindex="-1"><a class="header-anchor" href="#_1-take-the-2011-output-the-training-set-and-find-the-best-arma-p-q-model-for-the-data"><span>1. Take the 2011 output (the training set) and find the best <span style="color:orange;font-weight:bold;"><em>ARMA(p,q)</em></span> model for the data.</span></a></h4><figure><img src="'+f+'" alt="Question 1 - Output ACF" width="400" tabindex="0" loading="lazy"><figcaption>Question 1 - Output ACF</figcaption></figure><figure><img src="'+y+'" alt="Question 1 - Output PACF" width="400" tabindex="0" loading="lazy"><figcaption>Question 1 - Output PACF</figcaption></figure><figure><img src="'+b+'" alt="Possible ARMA Parameters" tabindex="0" loading="lazy"><figcaption>Possible ARMA Parameters</figcaption></figure><p>According to the parameters we got. We should compare the mean of squared error (MSE) and p-value of the parameters for each model to select a proper model, the smaller the better. The table below lists all the MSEs.</p><table><thead><tr><th></th><th>ARMA(3,3)</th><th>ARMA(4,1)</th><th>AR(4)</th><th>AR(3)</th><th>AR(2)</th></tr></thead><tbody><tr><td><strong>mean of square error</strong></td><td>overflow</td><td>85.89976922</td><td>24.75701538</td><td>24.76769416</td><td>24.92197907</td></tr></tbody></table><p>According to the mean of square error, the performance of AR(3) and AR(4) are similar, I select the <em><strong>AR(4)</strong></em> as the best model.</p><h4 id="_2-take-the-noise-from-that-model-and-check-its-sacf" tabindex="-1"><a class="header-anchor" href="#_2-take-the-noise-from-that-model-and-check-its-sacf"><span>2. Take the noise <span style="color:orange;font-weight:bold;"><em><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">Z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></em></span> from that model and check its <span style="color:orange;font-weight:bold;"><em>SACF</em></span>.</span></a></h4><p>I could get the residuals based on the model AR(4) get from the previous step, and name the residuals as Zt-AR(4). Then, we could see the SACF according to the ACF and PACF, like the pictures below.</p><p><img src="'+A+'" alt="ACF of residuals" width="400" loading="lazy"><br><img src="'+v+'" alt="PACF of residuals" width="400" loading="lazy"></p><p>According to the result from ACF and PACF, the <em><strong>Zt-AR(4)</strong></em> is not suit for using ARMA model to forecast.</p><h4 id="_3-calculate-and-show-that-it-has-the-arch-effect" tabindex="-1"><a class="header-anchor" href="#_3-calculate-and-show-that-it-has-the-arch-effect"><span>3. Calculate <span style="color:orange;font-weight:bold;"><em><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Z</mi><mi>t</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">Z_t^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></em></span> and show that it has the <span style="color:orange;font-weight:bold;"><em>ARCH</em></span> effect.</span></a></h4><p>For calculating the <em><strong>ARCH</strong></em> effect, it should be seperated into two parts, one for <strong>ARCH</strong> model, another for <strong>GARCH</strong> model.<br><em><strong>3.1</strong></em> Effect for ARCH model<br><img src="'+w+'" alt="ARCH SACF" width="400" loading="lazy"><br><img src="'+_+'" alt="ARCH SPACF" width="400" loading="lazy"><br> According to the SACF above, there is ARCH effect for ARCH model.</p><p><em><strong>3.2</strong></em> Effect for GARCH model<br><img src="'+x+'" alt="GARCH SACF" width="400" loading="lazy"><br><img src="'+q+'" alt="GARCH PSACF" width="400" loading="lazy"><br> According to the SACF above, there is also ARCH effect for GARCH model.</p><h4 id="_4-find-the-best-arch-or-garch-model-for-it" tabindex="-1"><a class="header-anchor" href="#_4-find-the-best-arch-or-garch-model-for-it"><span>4. Find the best <span style="color:orange;font-weight:bold;"><em>ARCH</em></span> or <span style="color:orange;font-weight:bold;"><em>GARCH</em></span> model for it.</span></a></h4><p>For this part, I try to find all possible ARCH and GARCH models, then try to compare the results for finalizing the model.<br> 4.1 ARCH model<br> According to the squared residuals for ARCH model, five <strong>AR</strong> models for the dataset could be found, the parameters like the picture below.<br><img src="'+k+'" alt="ARCH Parameters" loading="lazy"></p><p>According to the parameters above, the coverage rate could be calculated like the picture below.<br><img src="'+F+'" alt="Coverage Rate of ARCH Models" loading="lazy"><br> According to the result, all the coverage rate for the five model are similar, and they all approach 89.6% for score 1.96.</p><p>4.2 GARCH model<br> According to the squared residuals for GARCH model, 10 <strong>ARMA</strong> model for the dataset could be found, the parameters like the picture below.<br><img src="'+R+'" alt="GARCH Parameters" loading="lazy"></p><p>For some models will occurs negative values, which will lead to the specified model unavaliable. The coverage rate for each model like the picture below.<br><img src="'+z+'" alt="GARCH Coverage Rate" loading="lazy"></p><p>According to the result the best model for the residuals is <strong>GARCH(1,1)</strong>, the coverage could be 99.15%.</p><h4 id="_5-take-the-developed-models-for-the-output-and-also-for-the-noise-and-apply-them-to-the-2012-output-data" tabindex="-1"><a class="header-anchor" href="#_5-take-the-developed-models-for-the-output-and-also-for-the-noise-and-apply-them-to-the-2012-output-data"><span>5. Take the <span style="color:orange;font-weight:bold;">developed models</span> for <span style="color:orange;font-weight:bold;">the output</span> and also for <span style="color:orange;font-weight:bold;">the noise</span> and apply them to <span style="color:orange;font-weight:bold;">the 2012 output data</span>.</span></a></h4><p>According to previous steps, we got two models for the dataset. One is AR(4), another one is GARCH(1,1). All the models will apply to 2012 dataset. The result like the picture below.<br><img src="'+d+'" alt="2011 model apply to 2012 dataset 95%" loading="lazy"></p><h4 id="_6-evaluate-the-performance-of-the-models-for-one-step-ahead-forecasting-with-error-bounds-by-calculating-the-coverage-and-mean-prediction-interval-width-for-both-90-and-95-values" tabindex="-1"><a class="header-anchor" href="#_6-evaluate-the-performance-of-the-models-for-one-step-ahead-forecasting-with-error-bounds-by-calculating-the-coverage-and-mean-prediction-interval-width-for-both-90-and-95-values"><span>6. <span style="color:orange;font-weight:bold;">Evaluate the performance</span> of the models for <span style="color:orange;font-weight:bold;">one step ahead forecasting</span> with <span style="color:orange;font-weight:bold;">error bounds</span> by <span style="color:orange;font-weight:bold;">calculating the coverage and mean prediction interval width</span>, for both 90% and 95% values.</span></a></h4><p>The score of 90% is about 1.65 and the score of 95% is about 1.96. The 95% result like the picture below.</p><p>The 95% coverage result like the picture below.<br><img src="'+d+'" alt="2011 model apply to 2012 dataset 95%" loading="lazy"><br> The 90% coverage result like the picture below.<br><img src="'+C+'" alt="2011 model apply to 2012 dataset 90%" loading="lazy"><br> The statistical summary for the residuals of 2012 data like the picture below.<br><img src="'+j+'" alt="Statistical summary for residuals of 2012" loading="lazy"></p><p>According to the statistical summary, the standard deviation is about 5.05. The mean predictiion interval width of 95% coverage is about 18.87, for 90% coverage is about 15.89. The real coverage with 1.96 score is 94.75%, and 91.57% for 1.65 score. So we could get<br><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>94.75</mn><mi mathvariant="normal">%</mi><mo>≈</mo><mn>95</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">94.75\\% \\approx 95\\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">94.75%</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">95%</span></span></span></span><br><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>91.57</mn><mi mathvariant="normal">%</mi><mo>&gt;</mo><mn>90</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">91.57\\% \\gt 90\\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">91.57%</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">90%</span></span></span></span><br><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5.05</mn><mo>∗</mo><mn>1.96</mn><mo>∗</mo><mn>2</mn><mo>=</mo><mn>19.8</mn><mo>&gt;</mo><mn>18.87</mn></mrow><annotation encoding="application/x-tex">5.05*1.96*2 = 19.8 \\gt 18.87</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5.05</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1.96</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6835em;vertical-align:-0.0391em;"></span><span class="mord">19.8</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">18.87</span></span></span></span><br><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5.05</mn><mo>∗</mo><mn>1.65</mn><mo>∗</mo><mn>2</mn><mo>=</mo><mn>16.665</mn><mo>&gt;</mo><mn>15.89</mn></mrow><annotation encoding="application/x-tex">5.05*1.65*2 = 16.665 \\gt 15.89</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5.05</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1.65</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6835em;vertical-align:-0.0391em;"></span><span class="mord">16.665</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">15.89</span></span></span></span></p><p>The mean of prediction interval for 90% coverage is 15.88931, and 18.87457 for 90% coverage.</p><p>The results suggest that the model is quiet well for the dataset.</p><h4 id="_7-compare-the-results-with-constructing-the-prediction-intervals-by-using-the-appropriate-quantiles" tabindex="-1"><a class="header-anchor" href="#_7-compare-the-results-with-constructing-the-prediction-intervals-by-using-the-appropriate-quantiles"><span>7. <span style="color:orange;font-weight:bold;">Compare the results</span> with <span style="color:orange;font-weight:bold;">constructing the prediction intervals by using the appropriate quantiles</span>.</span></a></h4><p>The picture below is the prediction interval using GARCH and quantile method.<br><img src="'+M+'" alt="Quantile Error Bounds" loading="lazy"></p><p>The picture below is the result of quantile approach and GARCH.<br><img src="'+T+'" alt="Quantile Error Bounds and GARCH Error Bounds" loading="lazy"></p><p>According to the result ...</p><p><a href="https://medium.com/@qucit/a-simple-technique-to-estimate-prediction-intervals-for-any-regression-model-2dd73f630bcb" target="_blank" rel="noopener noreferrer">A simple technique to estimate prediction intervals for any regression model</a></p><h3 id="question-2" tabindex="-1"><a class="header-anchor" href="#question-2"><span>Question 2</span></a></h3><p>The <em><strong>tasks</strong></em> for this question are listed below.</p><h4 id="_1-test-the-months-december-january-february-july-august-for-normality" tabindex="-1"><a class="header-anchor" href="#_1-test-the-months-december-january-february-july-august-for-normality"><span>1. Test the months December, January, February, July, August for <span style="color:orange;font-weight:bold;">normality</span>.</span></a></h4><p>For normality test, the ppplot and histogram could be used for testing.</p><p><img src="'+D+'" alt="Histogram" loading="lazy"><br> According to the histogram all the distribution of the months are right skewed.<br><img src="'+J+'" alt="PP-Plot" loading="lazy"></p><p>According to the ppplot result, the p-value of July and August is greater than 0.05, we can not reject they follow the normal distribution, on the other hand the p-value of January February and December is less than 0.05, we reject the datasets of the three months follow normal distribution.</p><h4 id="_2-for-the-months-that-do-not-follow-a-normal-distribution-test-for-a-gamma-fit" tabindex="-1"><a class="header-anchor" href="#_2-for-the-months-that-do-not-follow-a-normal-distribution-test-for-a-gamma-fit"><span>2. For the months that <span style="color:orange;font-weight:bold;">do not follow a <span style="color:red;">normal distribution</span>, test for a <span style="color:red;">Gamma fit</span></span>.</span></a></h4><p>There are two steps for this question. The first step is to calculate the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> parameters. Another step is to get the distribution and visulize them. According to previous step, the datasets of Janarary and February will be processed.</p>',44)),o(l,{id:"305",data:[{id:"Excel"},{id:"Minitab"}]},{title0:n(({value:s,isActive:i})=>e[7]||(e[7]=[a("Excel")])),title1:n(({value:s,isActive:i})=>e[8]||(e[8]=[a("Minitab")])),tab0:n(({value:s,isActive:i})=>e[9]||(e[9]=[t("p",null,[a("2.1 Get the parameters for gamma"),t("br"),a(" The parameters "),t("span",{class:"katex"},[t("span",{class:"katex-mathml"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("semantics",null,[t("mrow",null,[t("mi",null,"α")]),t("annotation",{encoding:"application/x-tex"},"\\alpha")])])]),t("span",{class:"katex-html","aria-hidden":"true"},[t("span",{class:"base"},[t("span",{class:"strut",style:{height:"0.4306em"}}),t("span",{class:"mord mathnormal",style:{"margin-right":"0.0037em"}},"α")])])]),a(" and "),t("span",{class:"katex"},[t("span",{class:"katex-mathml"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("semantics",null,[t("mrow",null,[t("mi",null,"β")]),t("annotation",{encoding:"application/x-tex"},"\\beta")])])]),t("span",{class:"katex-html","aria-hidden":"true"},[t("span",{class:"base"},[t("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),t("span",{class:"mord mathnormal",style:{"margin-right":"0.05278em"}},"β")])])]),a(" are calculated like the picture below."),t("br"),t("img",{src:H,alt:"Parameters For Gamma",loading:"lazy"})],-1),t("p",null,[a("2.2 Visulize the distribution"),t("br"),t("img",{src:G,alt:"Gamma distribution for Jan and Feb",loading:"lazy"})],-1)])),tab1:n(({value:s,isActive:i})=>e[10]||(e[10]=[t("p",null,"2.1 Get the parameters for gamma",-1),t("figure",null,[t("img",{src:P,alt:"PP-Plot for Gamma",tabindex:"0",loading:"lazy"}),t("figcaption",null,"PP-Plot for Gamma")],-1),t("p",null,"2.2 Visualization",-1),t("figure",null,[t("img",{src:E,alt:"Gamma Distribution Fitting of Janurary",width:"400",tabindex:"0",loading:"lazy"}),t("figcaption",null,"Gamma Distribution Fitting of Janurary")],-1),t("figure",null,[t("img",{src:S,alt:"Gamma Distribution Fitting of February",width:"400",tabindex:"0",loading:"lazy"}),t("figcaption",null,"Gamma Distribution Fitting of February")],-1),t("figure",null,[t("img",{src:Q,alt:"Gamma Distribution Fitting of December",width:"400",tabindex:"0",loading:"lazy"}),t("figcaption",null,"Gamma Distribution Fitting of December")],-1)])),_:1}),e[29]||(e[29]=r('<h4 id="_3-test-december-january-february-for-correlation-and-july-august-separately" tabindex="-1"><a class="header-anchor" href="#_3-test-december-january-february-for-correlation-and-july-august-separately"><span>3. Test December, January, February for <span style="color:orange;font-weight:bold;">correlation</span>, and July, August separately.</span></a></h4><p><img src="'+Z+'" alt="Pearson Correlations in Minitab" loading="lazy"><br><img src="'+L+'" alt="PearsonCorrelationsMatrix" loading="lazy"><br> Correlations matrix</p><table><thead><tr><th></th><th>Jan</th><th>Feb</th><th>Jul</th><th>Aug</th></tr></thead><tbody><tr><td><strong>Feb</strong></td><td>-0.015</td><td></td><td></td><td></td></tr><tr><td><strong>Jul</strong></td><td>0.198</td><td>-0.070</td><td></td><td></td></tr><tr><td><strong>Aug</strong></td><td>-0.132</td><td>-0.000</td><td>-0.009</td><td></td></tr><tr><td><strong>Dec</strong></td><td>0.053</td><td>-0.196</td><td>-0.023</td><td>0.118</td></tr></tbody></table><p>According to the correlation matrix, there is almost no correlation among the months.</p><h4 id="_4-generate-1000-years-of-synthetic-december-january-february-add-the-months-to-get-seasonal-totals-and-generate-empirical-cdfs-for-the-totals-versus-the-cdfs-for-the-real-data" tabindex="-1"><a class="header-anchor" href="#_4-generate-1000-years-of-synthetic-december-january-february-add-the-months-to-get-seasonal-totals-and-generate-empirical-cdfs-for-the-totals-versus-the-cdfs-for-the-real-data"><span>4. Generate 1000 years of <span style="color:orange;font-weight:bold;">synthetic</span> December, January, February, add the months to <span style="color:orange;font-weight:bold;">get seasonal totals</span>, and <span style="color:orange;font-weight:bold;">generate empirical <span style="color:red;">CDFs</span> for the totals versus the <span style="color:red;">CDFs</span> for the real data</span>.</span></a></h4>',5)),o(l,{id:"438",data:[{id:"Excel"},{id:"Minitab"}]},{title0:n(({value:s,isActive:i})=>e[11]||(e[11]=[a("Excel")])),title1:n(({value:s,isActive:i})=>e[12]||(e[12]=[a("Minitab")])),tab0:n(({value:s,isActive:i})=>e[13]||(e[13]=[t("p",null,[t("img",{src:B,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:I,alt:"Alt text",loading:"lazy"})],-1),t("figure",null,[t("img",{src:O,alt:"Alt text",tabindex:"0",loading:"lazy"}),t("figcaption",null,"Alt text")],-1),t("p",null,[t("img",{src:V,alt:"Alt text",width:"600",loading:"lazy"}),t("br"),t("img",{src:N,alt:"Alt text",width:"600",loading:"lazy"}),t("br"),t("img",{src:U,alt:"Alt text",width:"600",loading:"lazy"})],-1)])),tab1:n(({value:s,isActive:i})=>e[14]||(e[14]=[t("p",null,[t("img",{src:W,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:$,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:K,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:X,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:Y,alt:"Alt text",loading:"lazy"})],-1)])),_:1}),e[30]||(e[30]=t("h4",{id:"_5-do-the-same-for-july-august",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#_5-do-the-same-for-july-august"},[t("span",null,[a("5. "),t("span",{style:{color:"orange","font-weight":"bold"}},"Do the same"),a(" for July, August.")])])],-1)),e[31]||(e[31]=t("p",null,"Because of July, August perhaps follow normal distribution, here we should generate normal distribution synthetic data.",-1)),o(l,{id:"462",data:[{id:'Excel<span style="color:red">(error)</span>'},{id:"Minitab"}]},{title0:n(({value:s,isActive:i})=>e[15]||(e[15]=[a("Excel"),t("span",{style:{color:"red"}},"(error)",-1)])),title1:n(({value:s,isActive:i})=>e[16]||(e[16]=[a("Minitab")])),tab0:n(({value:s,isActive:i})=>e[17]||(e[17]=[t("p",null,[t("img",{src:tt,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:at,alt:"Alt text",loading:"lazy"})],-1),t("figure",null,[t("img",{src:et,alt:"Alt text",tabindex:"0",loading:"lazy"}),t("figcaption",null,"Alt text")],-1),t("p",null,[t("img",{src:nt,alt:"Alt text",width:"600",loading:"lazy"}),t("br"),t("img",{src:st,alt:"Alt text",width:"600",loading:"lazy"})],-1)])),tab1:n(({value:s,isActive:i})=>e[18]||(e[18]=[t("p",null,[t("img",{src:it,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:ot,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:lt,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:rt,alt:"Alt text",loading:"lazy"})],-1)])),_:1}),e[32]||(e[32]=r('<h3 id="question-3" tabindex="-1"><a class="header-anchor" href="#question-3"><span>Question 3</span></a></h3><p>The <em><strong>tasks</strong></em> for this question are listed below.</p><h4 id="_1-take-the-monthly-rainfall-data-from-mtgambierrainfall-xlsx-and-model-the-seasonality-then-subtract-this-from-the-data" tabindex="-1"><a class="header-anchor" href="#_1-take-the-monthly-rainfall-data-from-mtgambierrainfall-xlsx-and-model-the-seasonality-then-subtract-this-from-the-data"><span>1. Take the monthly rainfall data from <code>MtGambierRainfall.xlsx</code> and <span style="color:orange;font-weight:bold;">model the seasonality</span>. Then <span style="color:orange;font-weight:bold;">subtract</span> this from the data.</span></a></h4><p>There are three steps for this question. The first step is to find the best frequencies, the second step is to find the proper parameters for seasonalities, and the last step is to visualize the seasonality result.</p><p>The picture below is the frequencies for the dataset. The 50 and 550 is the best for the dataset.<br><img src="'+mt+'" alt="Alt text" loading="lazy"></p><p>The picture below is the seasonality parameters using the frequencies got from step1. And also we got the final model and the residuals.<br><img src="'+dt+'" alt="Seasonality Parameters" loading="lazy"></p><p>The picture is the visualization for the final model of seasonality.<br><img src="'+ht+'" alt="Seasonality Visualization" loading="lazy"></p><h4 id="_2-use-exponential-smoothing-to-see-the-overall-trend-in-the-series-try-various-values-of-α-below-0-2" tabindex="-1"><a class="header-anchor" href="#_2-use-exponential-smoothing-to-see-the-overall-trend-in-the-series-try-various-values-of-α-below-0-2"><span>2. Use <span style="color:orange;font-weight:bold;">exponential smoothing</span> to see <span style="color:orange;font-weight:bold;">the overall trend</span> in the series - <span style="color:orange;font-weight:bold;">try various values of <code>α</code> below 0.2</span>.</span></a></h4>',8)),o(l,{id:"504",data:[{id:"Excel"},{id:"Minitab"}]},{title0:n(({value:s,isActive:i})=>e[19]||(e[19]=[a("Excel")])),title1:n(({value:s,isActive:i})=>e[20]||(e[20]=[a("Minitab")])),tab0:n(({value:s,isActive:i})=>[e[21]||(e[21]=t("p",null,[a("For this question, I will show the results of four values for the "),t("span",{class:"katex"},[t("span",{class:"katex-mathml"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("semantics",null,[t("mrow",null,[t("mi",null,"α")]),t("annotation",{encoding:"application/x-tex"},"\\alpha")])])]),t("span",{class:"katex-html","aria-hidden":"true"},[t("span",{class:"base"},[t("span",{class:"strut",style:{height:"0.4306em"}}),t("span",{class:"mord mathnormal",style:{"margin-right":"0.0037em"}},"α")])])]),a(". Details for the pictures below.")],-1)),u("![$\\alpha = 0.002$](/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_data.png =400x)smoothing_data"),e[22]||(e[22]=t("figure",null,[t("img",{src:pt,alt:"Alt text",tabindex:"0",loading:"lazy"}),t("figcaption",null,"Alt text")],-1)),e[23]||(e[23]=t("p",null,[t("img",{src:ct,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:gt,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:ut,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:ft,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:yt,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:bt,alt:"Alt text",loading:"lazy"})],-1)),e[24]||(e[24]=t("p",null,[t("a",{href:"https://www.wallstreetmojo.com/exponential-smoothing-in-excel/",target:"_blank",rel:"noopener noreferrer"},"Reference")],-1))]),tab1:n(({value:s,isActive:i})=>e[25]||(e[25]=[t("p",null,[t("img",{src:At,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:vt,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:wt,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:_t,alt:"Alt text",loading:"lazy"}),t("br"),t("img",{src:xt,alt:"Alt text",loading:"lazy"})],-1)])),_:1}),e[33]||(e[33]=r('<h4 id="_3-find-the-trend-for-the-whole-series-for-the-smoothed-data-and-then-find-the-trends-for-any-sections-that-you-think-display-differing-characteristics" tabindex="-1"><a class="header-anchor" href="#_3-find-the-trend-for-the-whole-series-for-the-smoothed-data-and-then-find-the-trends-for-any-sections-that-you-think-display-differing-characteristics"><span>3. <span style="color:orange;font-weight:bold;">Find the trend</span> for the whole series for <span style="color:orange;font-weight:bold;">the smoothed data</span>, and then <span style="color:orange;font-weight:bold;">find the trends for any sections that you think display differing characteristics</span>.</span></a></h4><p>I will set the parameter <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> equals 0.02 of smoothed data. And then to process the smoothed data. There are 3 steps to do. The first step is to find the trend of the whole dataset. The second step is to split the dataset into multiple sections, and the last step is to find the trends for each section.</p><p>3.1 Find the trend of whole dataset.<br> I use the univariate linear regression to model the trend. The result like the picture below.<br><img src="'+qt+'" alt="Trend of whole dataset" loading="lazy"><br> 3.2 Split whole dataset into multiple sections.<br> Accoriding the visualization of the dataset, the dataset could be split into two sections, the first section(from the begining to 60) rise rapidly, and the second section oscillate around a variable. So the dataset could be split into two sections like the pciture below.<br><img src="'+kt+'" alt="Alt text" loading="lazy"><br> 3.3 find the trends for the two sections<br> The trends of the two sections like the picture belowl<br><img src="'+Ft+'" alt="Alt text" loading="lazy"></p><h4 id="_4-take-the-data-for-the-month-of-december-and-the-annual-mean-temperature-from-mtgambierbymonthstemperature-xlsx-and-find-the-trend-over-time" tabindex="-1"><a class="header-anchor" href="#_4-take-the-data-for-the-month-of-december-and-the-annual-mean-temperature-from-mtgambierbymonthstemperature-xlsx-and-find-the-trend-over-time"><span>4. Take the data for the <span style="color:orange;font-weight:bold;">month of December</span> and the <span style="color:orange;font-weight:bold;">Annual mean temperature</span> from <code>MtGambierByMonthsTemperature.xlsx</code> and <span style="color:orange;font-weight:bold;">find the trend over time</span>.</span></a></h4><figure><img src="'+Rt+'" alt="Alt text" tabindex="0" loading="lazy"><figcaption>Alt text</figcaption></figure><h4 id="_5-how-much-has-the-mean-temperature-changed-over-time-in-each-case" tabindex="-1"><a class="header-anchor" href="#_5-how-much-has-the-mean-temperature-changed-over-time-in-each-case"><span>5. How much has <span style="color:orange;font-weight:bold;">the mean temperature changed</span> over time in each case?</span></a></h4><p>According to the result from the last step, the mean temperature changed over time should be calculated via the linear regression parameters with the whole 73 years on this dataset.<br> The temperature changed over time for December should be,<br><span style="font-weight:bold;">a * year = 0.028117 * 73 = <span style="color:red;">2.05</span></span><br> The temperature changed over time for December should be,<br><span style="font-weight:bold;">a * year = 0.018201 * 73 = <span style="color:red;">1.32</span></span></p>',7))])}const Jt=p(zt,[["render",jt],["__file","assignment2.html.vue"]]),Ht=JSON.parse(`{"path":"/study/unisa/2023SP5/AdvancedAnalyticTechniques1/assignment2.html","title":"Assignment 2","lang":"en-US","frontmatter":{"title":"Assignment 2","index":false,"icon":"/assets/icon/common/assignment.svg","icon-size":"4rem","author":"Haiyue","date":"2023-10-02T00:00:00.000Z","sidebar":false,"category":["Assignment"],"description":"Requirements Instructions Requirements Analysis Details Access Resources Access Resources-FR All works below Question 1 1. Take the 2011 output (the training set) and find the b...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://jet-w.github.io/study/unisa/2023SP5/AdvancedAnalyticTechniques1/assignment2.html"}],["meta",{"property":"og:site_name","content":"Haiyue's Blog"}],["meta",{"property":"og:title","content":"Assignment 2"}],["meta",{"property":"og:description","content":"Requirements Instructions Requirements Analysis Details Access Resources Access Resources-FR All works below Question 1 1. Take the 2011 output (the training set) and find the b..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_acf.png =400x"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"article:author","content":"Haiyue"}],["meta",{"property":"article:published_time","content":"2023-10-02T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Assignment 2\\",\\"image\\":[\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_acf.png =400x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_pacf.png =400x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.ARMA_Params.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_zt_acf.png =400x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_zt_pacf.png =400x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.arch_effect_acf.png =400x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.arch_effect_pacf.png =400x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_zt_squared_acf.png =400x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.output_zt_squared_pacf.png =400x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.ARCH_Params.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.ARCH_CoverageRate.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.GARCH_Params.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.GARCH_CoverageRate.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.Model2011To2012.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.Model2011To2012.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.Model2011To2012.acc90.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.statisticalfor2012Zt.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.quantile.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q1.quantile_ret.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.histogram.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.pp-plot.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.ParametersForGamma.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.gammadistribution.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.gammatest.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.gamma-Jan.png =400x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.gamma-Feb.png =400x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.gamma-Dec.png =400x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.correlations.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.PearsonCorrelationsMatrix.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.generate_data_JanFebDec.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.maxmin_JanFebDec.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.freq_JanFebDec.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.accfreq_Jan.png =600x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.accfreq_Feb.png =600x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.accfreq_Dec.png =600x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.pp-plot_JanFebDec.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_Jan.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_Feb.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_Dec.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_JanFebDec.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.generate_data_JulAug.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.maxmin_JulAug.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.freq_JulAug.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.accfreq_Jul.png =600x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.accfreq_Aug.png =600x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ppplot_JulyAug.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_JulyAug.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_Jul.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q2.minitab.ECDF_Aug.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.DFT_power.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.seasonality.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.seasonality_visualization.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_data.png =400x\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_data.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_alpha_0.02.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_alpha_0.05.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_alpha_0.1.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_alpha_0.15.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_alpha_0.2.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.total_trends.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.minitab.smoothing_alpha_0.02.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.minitab.smoothing_alpha_0.05.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.minitab.smoothing_alpha_0.1.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.minitab.smoothing_alpha_0.15.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.minitab.smoothing_alpha_0.2.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_total_trend.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.smoothing_multisection.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.excel.VisualizationOfTrends.png\\",\\"https://jet-w.github.io/data/unisa/AdvancedAnalytic1/assignment2/img/q3.temperature-trend.png\\"],\\"datePublished\\":\\"2023-10-02T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Haiyue\\"}]}"]]},"headers":[{"level":2,"title":"Requirements","slug":"requirements","link":"#requirements","children":[]},{"level":2,"title":"Requirements Analysis","slug":"requirements-analysis","link":"#requirements-analysis","children":[]},{"level":2,"title":"All works below","slug":"all-works-below","link":"#all-works-below","children":[{"level":3,"title":"Question 1","slug":"question-1","link":"#question-1","children":[{"level":4,"title":"1. Take the 2011 output (the training set) and find the best ARMA(p,q) model for the data.","slug":"_1-take-the-2011-output-the-training-set-and-find-the-best-arma-p-q-model-for-the-data","link":"#_1-take-the-2011-output-the-training-set-and-find-the-best-arma-p-q-model-for-the-data","children":[]},{"level":4,"title":"2. Take the noise  from that model and check its SACF.","slug":"_2-take-the-noise-from-that-model-and-check-its-sacf","link":"#_2-take-the-noise-from-that-model-and-check-its-sacf","children":[]},{"level":4,"title":"3. Calculate  and show that it has the ARCH effect.","slug":"_3-calculate-and-show-that-it-has-the-arch-effect","link":"#_3-calculate-and-show-that-it-has-the-arch-effect","children":[]},{"level":4,"title":"4. Find the best ARCH or GARCH model for it.","slug":"_4-find-the-best-arch-or-garch-model-for-it","link":"#_4-find-the-best-arch-or-garch-model-for-it","children":[]},{"level":4,"title":"5. Take the developed models for the output and also for the noise and apply them to the 2012 output data.","slug":"_5-take-the-developed-models-for-the-output-and-also-for-the-noise-and-apply-them-to-the-2012-output-data","link":"#_5-take-the-developed-models-for-the-output-and-also-for-the-noise-and-apply-them-to-the-2012-output-data","children":[]},{"level":4,"title":"6. Evaluate the performance of the models for one step ahead forecasting with error bounds by calculating the coverage and mean prediction interval width, for both 90% and 95% values.","slug":"_6-evaluate-the-performance-of-the-models-for-one-step-ahead-forecasting-with-error-bounds-by-calculating-the-coverage-and-mean-prediction-interval-width-for-both-90-and-95-values","link":"#_6-evaluate-the-performance-of-the-models-for-one-step-ahead-forecasting-with-error-bounds-by-calculating-the-coverage-and-mean-prediction-interval-width-for-both-90-and-95-values","children":[]},{"level":4,"title":"7. Compare the results with constructing the prediction intervals by using the appropriate quantiles.","slug":"_7-compare-the-results-with-constructing-the-prediction-intervals-by-using-the-appropriate-quantiles","link":"#_7-compare-the-results-with-constructing-the-prediction-intervals-by-using-the-appropriate-quantiles","children":[]}]},{"level":3,"title":"Question 2","slug":"question-2","link":"#question-2","children":[{"level":4,"title":"1. Test the months December, January, February, July, August for normality.","slug":"_1-test-the-months-december-january-february-july-august-for-normality","link":"#_1-test-the-months-december-january-february-july-august-for-normality","children":[]},{"level":4,"title":"2. For the months that do not follow a normal distribution, test for a Gamma fit.","slug":"_2-for-the-months-that-do-not-follow-a-normal-distribution-test-for-a-gamma-fit","link":"#_2-for-the-months-that-do-not-follow-a-normal-distribution-test-for-a-gamma-fit","children":[]},{"level":4,"title":"3. Test December, January, February for correlation, and July, August separately.","slug":"_3-test-december-january-february-for-correlation-and-july-august-separately","link":"#_3-test-december-january-february-for-correlation-and-july-august-separately","children":[]},{"level":4,"title":"4. Generate 1000 years of synthetic December, January, February, add the months to get seasonal totals, and generate empirical CDFs for the totals versus the CDFs for the real data.","slug":"_4-generate-1000-years-of-synthetic-december-january-february-add-the-months-to-get-seasonal-totals-and-generate-empirical-cdfs-for-the-totals-versus-the-cdfs-for-the-real-data","link":"#_4-generate-1000-years-of-synthetic-december-january-february-add-the-months-to-get-seasonal-totals-and-generate-empirical-cdfs-for-the-totals-versus-the-cdfs-for-the-real-data","children":[]},{"level":4,"title":"5. Do the same for July, August.","slug":"_5-do-the-same-for-july-august","link":"#_5-do-the-same-for-july-august","children":[]}]},{"level":3,"title":"Question 3","slug":"question-3","link":"#question-3","children":[{"level":4,"title":"1. Take the monthly rainfall data from MtGambierRainfall.xlsx and model the seasonality. Then subtract this from the data.","slug":"_1-take-the-monthly-rainfall-data-from-mtgambierrainfall-xlsx-and-model-the-seasonality-then-subtract-this-from-the-data","link":"#_1-take-the-monthly-rainfall-data-from-mtgambierrainfall-xlsx-and-model-the-seasonality-then-subtract-this-from-the-data","children":[]},{"level":4,"title":"2. Use exponential smoothing to see the overall trend in the series - try various values of α below 0.2.","slug":"_2-use-exponential-smoothing-to-see-the-overall-trend-in-the-series-try-various-values-of-α-below-0-2","link":"#_2-use-exponential-smoothing-to-see-the-overall-trend-in-the-series-try-various-values-of-α-below-0-2","children":[]},{"level":4,"title":"3. Find the trend for the whole series for the smoothed data, and then find the trends for any sections that you think display differing characteristics.","slug":"_3-find-the-trend-for-the-whole-series-for-the-smoothed-data-and-then-find-the-trends-for-any-sections-that-you-think-display-differing-characteristics","link":"#_3-find-the-trend-for-the-whole-series-for-the-smoothed-data-and-then-find-the-trends-for-any-sections-that-you-think-display-differing-characteristics","children":[]},{"level":4,"title":"4. Take the data for the month of December and the Annual mean temperature from MtGambierByMonthsTemperature.xlsx and find the trend over time.","slug":"_4-take-the-data-for-the-month-of-december-and-the-annual-mean-temperature-from-mtgambierbymonthstemperature-xlsx-and-find-the-trend-over-time","link":"#_4-take-the-data-for-the-month-of-december-and-the-annual-mean-temperature-from-mtgambierbymonthstemperature-xlsx-and-find-the-trend-over-time","children":[]},{"level":4,"title":"5. How much has the mean temperature changed over time in each case?","slug":"_5-how-much-has-the-mean-temperature-changed-over-time-in-each-case","link":"#_5-how-much-has-the-mean-temperature-changed-over-time-in-each-case","children":[]}]}]}],"readingTime":{"minutes":9.48,"words":2844},"filePathRelative":"study/unisa/2023SP5/AdvancedAnalyticTechniques1/assignment2.md","localizedDate":"October 2, 2023","excerpt":"<h2>Requirements</h2>\\n<details class=\\"hint-container details\\"><summary>Instructions</summary>\\n</details>\\n<h2>Requirements Analysis</h2>\\n<details class=\\"hint-container details\\"><summary>Details</summary>\\n<p><a href=\\"https://lo.unisa.edu.au/mod/folder/view.php?id=3409063\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Access Resources</a><br>\\n<a href=\\"/data/unisa/AdvancedAnalytic1/assignment2/assign2_report.docx\\">Access Resources-FR</a></p>\\n</details>","autoDesc":true}`);export{Jt as comp,Ht as data};
