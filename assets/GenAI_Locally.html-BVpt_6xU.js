import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as o,o as l}from"./app-4B8A9UGj.js";const t={};function n(s,e){return l(),a("div",null,e[0]||(e[0]=[o('<h2 id="previous-work" tabindex="-1"><a class="header-anchor" href="#previous-work"><span>Previous work</span></a></h2><p><a href="https://colab.research.google.com/drive/11vliXWcRSSwZe027Z5wMn7ne7TH3H64j?usp=sharing" target="_blank" rel="noopener noreferrer">Event Ranking</a></p><h2 id="resources" tabindex="-1"><a class="header-anchor" href="#resources"><span>Resources</span></a></h2><p><a href="https://lachieslifestyle.com/2023/07/29/how-to-install-llama-2/" target="_blank" rel="noopener noreferrer">How to use LLama Model locally</a><br><a href="">lmstudio.ai</a><br><a href="https://aws.amazon.com/blogs/aws/amazon-bedrock-now-provides-access-to-llama-2-chat-13b-model/" target="_blank" rel="noopener noreferrer">Llama 2 Chat 13B now supported in Amazon Bedrock</a></p><p><a href="https://pypi.org/project/llama-server/" target="_blank" rel="noopener noreferrer">llama-server</a><br><a href="https://www.philschmid.de/inferentia2-llama-7b" target="_blank" rel="noopener noreferrer">Deploy Llama 2 7B on AWS inferentia2 with Amazon SageMaker</a></p>',5)]))}const p=r(t,[["render",n],["__file","GenAI_Locally.html.vue"]]),h=JSON.parse('{"path":"/work/narrativesummarization/GenAI_Locally.html","title":"06. Local AIGC","lang":"en-US","frontmatter":{"title":"06. Local AIGC","index":true,"icon":"circle-dot","author":"Haiyue","date":"2023-11-14T00:00:00.000Z","category":["work"],"feed":false,"seo":false,"gitInclude":[],"head":[]},"headers":[{"level":2,"title":"Previous work","slug":"previous-work","link":"#previous-work","children":[]},{"level":2,"title":"Resources","slug":"resources","link":"#resources","children":[]}],"readingTime":{"minutes":0.26,"words":79},"filePathRelative":"work/narrativesummarization/GenAI_Locally.md","localizedDate":"November 14, 2023"}');export{p as comp,h as data};
